{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafa935c-cb05-47ea-96e8-935365bbdcb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import astroid\n",
    "import ast\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from radon.complexity import cc_visit_ast\n",
    "from radon.metrics import h_visit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca3c2b58-df31-4a0e-8304-e5b707464cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_code(data, keep_comments=False):\n",
    "    if keep_comments:\n",
    "        data[\"cleaned_code\"] = data[\"code\"].apply(lambda row: \"\\n\".join([value for value in row[4:].replace(\"\\n    \", \"\\n\").splitlines()]))\n",
    "    else:\n",
    "        data[\"cleaned_code\"] = data[\"code\"].apply(lambda row: (re.sub(r\" *#.*\\n\", \"\\n\", \"\\n\".join([value for value in row[4:].replace(\"\\n    \", \"\\n\").splitlines() if len(value.lstrip())==0 or value.lstrip()[0] != \"#\"]))).lstrip())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fe055d0-a939-442a-9feb-5ab57436f6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(data, code_column=\"cleaned_code\", has_comments=False):\n",
    "    data = data.copy()\n",
    "    data[\"num_chars\"] = data[code_column].str.len()\n",
    "    data[\"num_lines\"] = data[code_column].apply(lambda row: len(row.splitlines()))\n",
    "    data[\"avg_line_length\"] = data[code_column].apply(lambda row: np.mean([len(line) for line in row.splitlines() if len(line)>0]))\n",
    "    data[\"max_line_length\"] = data[code_column].apply(lambda row: max([len(line) for line in row.splitlines() if len(line)>0]))\n",
    "    data[\"num_digits\"] = (data[code_column].apply(lambda row: len([value for value in row if value.isdigit()])))\n",
    "    data[\"num_empty_lines\"] = data[code_column].str.count(\"\\n\\n\")\n",
    "    data[\"num_whitespace\"] = data[\"code\"].str.count(\" \")\n",
    "    data[\"num_indents\"] = data[\"code\"].apply(lambda row: sum([len(line) - len(line.lstrip()) for line in row.splitlines()]))\n",
    "    data[\"num_method_declarations\"] = data[code_column].str.count(\"def\")\n",
    "    data[\"num_break\"] = data[code_column].str.count(\"break\")\n",
    "    data[\"num_continue\"] = data[code_column].str.count(\"continue\")\n",
    "    data[\"num_with\"] = data[code_column].str.count(\"with\")\n",
    "    data[\"num_assert\"] = data[code_column].str.count(\"assert\")\n",
    "    data[\"num_except\"] = data[code_column].str.count(\"except\")\n",
    "    data[\"num_not\"] = data[code_column].str.count(\"not\")\n",
    "    data[\"num_or\"] = data[code_column].str.count(\"or\")\n",
    "    data[\"num_and\"] = data[code_column].str.count(\"and\")\n",
    "    data[\"num_none\"] = data[code_column].str.count(\"None\")\n",
    "    data[\"num_in\"] = data[code_column].str.count(\" in \")\n",
    "    data[\"num_yield\"] = data[code_column].str.count(\"yield\")\n",
    "    data[\"num_pass\"] = data[code_column].str.count(\"pass\")\n",
    "    data[\"num_raise\"] = data[code_column].str.count(\"raise\")\n",
    "    data[\"num_for\"] = data[code_column].str.count(\"for\")\n",
    "    data[\"num_while\"] = data[code_column].str.count(\"while\")\n",
    "    data[\"num_comparators\"] = data[code_column].str.count(\"==\") + data[code_column].str.count(\"<\") + data[code_column].str.count(\">\") + data[code_column].str.count(\"!=\")\n",
    "    data[\"num_local_vars\"] = data[code_column].apply(lambda code: len({node.targets[0].as_string() for node in astroid.parse(code).nodes_of_class(astroid.Assign)}))\n",
    "    data[\"avg_name_length\"] = data[code_column].apply(lambda code: np.mean(list({len(node.targets[0].as_string()) for node in astroid.parse(code).nodes_of_class(astroid.Assign)})))\n",
    "    data[\"max_name_length\"] = data[code_column].apply(lambda code: max(list({len(node.targets[0].as_string()) for node in astroid.parse(code).nodes_of_class(astroid.Assign)}), default=0))\n",
    "    data[\"num_function_calls\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.Call))))\n",
    "    data[\"num_loops\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.For))) + len(list(astroid.parse(code).nodes_of_class(astroid.While))))\n",
    "    data[\"num_if_statements\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.If))))\n",
    "    data[\"num_return_statements\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.Return))))\n",
    "    data[\"num_exceptions_raised\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.Raise))))\n",
    "    data[\"num_list_comprehensions\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.ListComp))))\n",
    "    data[\"num_dict_comprehensions\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.DictComp))))\n",
    "    data[\"num_set_comprehensions\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.SetComp))))\n",
    "    data[\"num_imported_modules\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.Import))))\n",
    "    data[\"num_list_operations\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.List))))\n",
    "    data[\"num_dict_operations\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.Dict))))\n",
    "    data[\"num_set_operations\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.Set))))\n",
    "    data[\"num_lambda_functions\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.Lambda))))\n",
    "    data[\"num_generator_expressions\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.GeneratorExp))))\n",
    "    data[\"num_attributes_accessed\"] = data[code_column].apply(lambda code: len(list(astroid.parse(code).nodes_of_class(astroid.Attribute))))\n",
    "    data[\"cyclomatic_complexity\"] = data[code_column].apply(lambda code: sum(item.complexity for item in cc_visit_ast(ast.parse(f\"def temp():\\n\" + \"\\n\".join(f\"    {line}\" for line in code.split(\"\\n\"))))))\n",
    "    data[\"halstead_operators\"] = data[code_column].apply(lambda code: h_visit(ast.parse(f\"def temp():\\n\" + \"\\n\".join(f\"    {line}\" for line in code.split(\"\\n\")))).total.h1)\n",
    "    data[\"halstead_operands\"] = data[code_column].apply(lambda code: h_visit(ast.parse(f\"def temp():\\n\" + \"\\n\".join(f\"    {line}\" for line in code.split(\"\\n\")))).total.h2)\n",
    "    data[\"halstead_length\"] = data[code_column].apply(lambda code: h_visit(ast.parse(f\"def temp():\\n\" + \"\\n\".join(f\"    {line}\" for line in code.split(\"\\n\")))).total.calculated_length)\n",
    "    data[\"halstead_volume\"] = data[code_column].apply(lambda code: h_visit(ast.parse(f\"def temp():\\n\" + \"\\n\".join(f\"    {line}\" for line in code.split(\"\\n\")))).total.volume)\n",
    "    data[\"halstead_difficulty\"] = data[code_column].apply(lambda code: h_visit(ast.parse(f\"def temp():\\n\" + \"\\n\".join(f\"    {line}\" for line in code.split(\"\\n\")))).total.difficulty)\n",
    "    data[\"halstead_effort\"] = data[code_column].apply(lambda code: h_visit(ast.parse(f\"def temp():\\n\" + \"\\n\".join(f\"    {line}\" for line in code.split(\"\\n\")))).total.effort)\n",
    "    data[\"halstead_time\"] = data[code_column].apply(lambda code: h_visit(ast.parse(f\"def temp():\\n\" + \"\\n\".join(f\"    {line}\" for line in code.split(\"\\n\")))).total.time)\n",
    "    data[\"halstead_bugs\"] = data[code_column].apply(lambda code: h_visit(ast.parse(f\"def temp():\\n\" + \"\\n\".join(f\"    {line}\" for line in code.split(\"\\n\")))).total.bugs)\n",
    "    data = data.fillna(0)\n",
    "    \n",
    "    if has_comments:\n",
    "        data[\"num_comments\"] = data[code_column].str.count(\"#\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70cd11ca-e01b-49d8-92f8-6edd3cb6b674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_bow(data, code_column=\"cleaned_code\"):\n",
    "    vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b|\\S\", lowercase=True)\n",
    "    bag_of_words = vectorizer.fit_transform(list(data[\"cleaned_code\"]))\n",
    "    bow_data = pd.DataFrame(bag_of_words.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return bow_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76a37d-6090-49fb-9c80-0ee07e2e3e02",
   "metadata": {},
   "source": [
    "## Classify the sample data using a feature approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff8511-dd23-42e3-9e72-04ae5982fe2f",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "364a7740-43db-48a2-9768-a5c024cc38ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_data = pd.read_csv(\"data/sample_data.csv\")\n",
    "sample_data = sample_data[[\"Problem_ID\", \"Canonical_Solution\", \"GPT4_Solution\"]]\n",
    "sample_data.columns = [\"problem_id\", \"human\", \"gpt4\"]\n",
    "sample_data = pd.melt(sample_data, id_vars=\"problem_id\", var_name=\"source\", value_name=\"code\")\n",
    "sample_data_comments = sample_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45e9366e-4633-49e8-b7ec-2441624f39ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_data = format_code(sample_data)\n",
    "sample_data = extract_features(sample_data)\n",
    "sample_data.to_csv(\"data/sample_data_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fa165a7-387e-4468-a0c4-36b0a30d60c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_data_comments = format_code(sample_data_comments, keep_comments=True)\n",
    "sample_data_comments = extract_features(sample_data_comments, has_comments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac2c3bf4-cb4e-4ae7-b92a-2ebd178540db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 52\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features:\", len(list(sample_data.columns[4:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86dd80b5-9275-419a-a7b9-cfeae66e6584",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 52.5 %\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sample_data[features].values, sample_data[\"source\"], test_size=0.3, random_state=0)\n",
    "forest_model = RandomForestClassifier(random_state=0)\n",
    "forest_model.fit(X_train, y_train)\n",
    "print(\"Accuracy on test set:\", np.round(accuracy_score(y_test, forest_model.predict(X_test)), 3)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c731ee29-b7d8-4372-9932-e5320be1c18b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 59.599999999999994 %\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sample_data_comments[features].values, sample_data_comments[\"source\"], test_size=0.3, random_state=0)\n",
    "forest_model = RandomForestClassifier(random_state=0)\n",
    "forest_model.fit(X_train, y_train)\n",
    "print(\"Accuracy on test set:\", np.round(accuracy_score(y_test, forest_model.predict(X_test)), 3)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442af22-4d27-4f94-b25f-68822b5f3cb6",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e1db6c6-8f21-45ef-818b-629b8fb07660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bow_data = create_bow(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "564b7339-2c5b-442e-bd68-cd3fe1a7598c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_data.values, sample_data[\"source\"], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0a1d339-1a85-42e7-b2a0-fde355965477",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 44.4 %\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestClassifier(random_state=0, n_estimators=160, max_features=\"sqrt\")\n",
    "forest_model.fit(X_train, y_train)\n",
    "print(\"Accuracy on test set:\", np.round(accuracy_score(y_test, forest_model.predict(X_test)), 3)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9eaed9-c4ca-4406-8aa5-ec6b1d7ec8cf",
   "metadata": {},
   "source": [
    "### Features and bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874a260a-5d55-4123-8911-b830a2bd398d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_bow_data = pd.concat([sample_data, bow_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f52efd5-fdab-4c27-a139-3676fef36bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sample_bow_data.iloc[:,4:].values, sample_bow_data[\"source\"], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d71f1c-bacb-45ee-bd58-58592384f08a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 51.5 %\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestClassifier(random_state=0, n_estimators=160, max_features=\"sqrt\")\n",
    "forest_model.fit(X_train, y_train)\n",
    "print(\"Accuracy on test set:\", np.round(accuracy_score(y_test, forest_model.predict(X_test)), 3)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86392894-071e-4319-992f-3f2b7910c4fd",
   "metadata": {},
   "source": [
    "## Classify the larger dataset using a feature approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e41a001-8c1b-4df5-bc96-29bb608caa91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "big_data = []\n",
    "with open(\"data/all_passed_results.jsonl\", 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        big_data.append(json_obj)\n",
    "big_data = pd.DataFrame(big_data)\n",
    "\n",
    "failed_parses = []\n",
    "for i in range(len(big_data)):\n",
    "    try:\n",
    "        astroid.parse(big_data.loc[i, \"code\"])\n",
    "    except:\n",
    "        failed_parses.append(i)\n",
    "        \n",
    "big_data = big_data.drop(failed_parses, axis=0).reset_index(drop=True)\n",
    "big_data[\"is_gpt\"] = big_data[\"is_gpt\"].replace({True:\"gpt\", False:\"human\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b18c9515-559d-4bd9-89bc-cc9cc9dd0153",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPS     69221\n",
       "MBPPD     1493\n",
       "HED        202\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_data[\"source\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f52ce7-9702-41d0-804e-7ec188bf6006",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human    96.353432\n",
       "gpt       3.646568\n",
       "Name: is_gpt, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_data[\"is_gpt\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "192d9ac2-9053-4e76-9dba-1bdec995d91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "big_data = extract_features(big_data, \"code\", has_comments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae8ea563-1a2c-4b6e-b5ab-3fea83b29365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "big_features = list(big_data.columns[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e69d2ea-f186-4919-9074-b0968b6202ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 96.39999999999999 %\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(big_data[big_features].values, big_data[\"is_gpt\"], test_size=0.3, random_state=0)\n",
    "forest_model = RandomForestClassifier(random_state=0,  class_weight=\"balanced\")\n",
    "forest_model.fit(X_train, y_train)\n",
    "print(\"Accuracy on test set:\", np.round(accuracy_score(y_test, forest_model.predict(X_test)), 3)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35c15183-37cc-4a23-8f01-34faad800c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt      94.310999\n",
       "human     0.078110\n",
       "Name: is_gpt, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of each source that was misclassified\n",
    "# 93% of the gpt solutions were wrongly classified as human\n",
    "misclassified = y_test != forest_model.predict(X_test)\n",
    "big_data.loc[misclassified[misclassified].index, :][\"is_gpt\"].value_counts() / big_data.loc[y_test.index, :][\"is_gpt\"].value_counts()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b08cc-c3d1-4546-9cea-7f6ad155438e",
   "metadata": {},
   "source": [
    "## Classify the larger dataset with SMOTE using a feature approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "edc21bec-036e-4aa4-8cda-44de0b504473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(big_data[big_features].values, big_data[\"is_gpt\"], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cef15d24-cdf9-410d-b55d-b8e5c0bdf203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_gpt = big_data[big_data[\"is_gpt\"]!=\"human\"].sample(frac=0.7, random_state=0)\n",
    "test_gpt = big_data[big_data[\"is_gpt\"]!=\"human\"].drop(train_gpt.index)\n",
    "\n",
    "test_human = big_data[big_data[\"is_gpt\"]==\"human\"].sample(len(test_gpt), random_state=0)\n",
    "train_human = big_data[big_data[\"is_gpt\"]==\"human\"].drop(test_human.index)\n",
    "\n",
    "train = pd.concat([train_gpt, train_human])\n",
    "test = pd.concat([test_gpt, test_human])\n",
    "\n",
    "X_train = train[big_features].values\n",
    "X_test = test[big_features].values\n",
    "y_train = train[\"is_gpt\"]\n",
    "y_test = test[\"is_gpt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1d29531-8837-4e9a-a719-381110da79a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a5dd498a-ec76-44d1-a506-c5ed7854a16b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt      67554\n",
       "human    67554\n",
       "Name: is_gpt, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f12b88e2-b752-4150-b4bb-e1b0ff6a72df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 55.50000000000001 %\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestClassifier(random_state=0)\n",
    "forest_model.fit(X_res, y_res)\n",
    "print(\"Accuracy on test set:\", np.round(accuracy_score(y_test, forest_model.predict(X_test)), 3)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91373454-504f-4a41-96bd-a70724d8987d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt      88.917526\n",
       "human          NaN\n",
       "Name: is_gpt, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 87% of the gpt solutions were misclassified\n",
    "misclassified = y_test != forest_model.predict(X_test)\n",
    "big_data.loc[misclassified[misclassified].index, :][\"is_gpt\"].value_counts() / big_data.loc[y_test.index, :][\"is_gpt\"].value_counts()*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
